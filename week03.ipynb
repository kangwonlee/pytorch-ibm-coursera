{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kangwonlee/pytorch-ibm-coursera/blob/main/week03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello PyTorch üëãüèª\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "references\n",
    "* https://www.coursera.org/learn/deep-neural-networks-with-pytorch/\n",
    "* https://github.com/damounayman/Deep-Neural-Networks-with-PyTorch/blob/main/Week1/1D_tensors.ipynb\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## week 3\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Multiple Linear Regression Prediction\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\mathbb{y}_{n \\times 1} = \\mathbb{X}_{n \\times m}\\mathbb{w}_{m \\times 1} + b_{-1\\times 1}\n",
    "$$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, -1 of $b_{-1 \\times 1}$ means the `numpy` broadcasting.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Linear Class\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1)\n",
    "model = torch.nn.Linear(in_features=2, out_features=1)\n",
    "\n",
    "list(model.parameters())\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.state_dict()\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Forward\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "X = torch.tensor([1.0, 3.0])\n",
    "yhat = model(X)\n",
    "yhat\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Custom Modules\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "class LR(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(LR, self).__init__()\n",
    "    self.linear = torch.nn.Linear(input_size, output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.linear(x)\n",
    "\n",
    "model = LR(input_size=2, output_size=1)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X = torch.tensor(\n",
    "    [\n",
    "        [1.0, 1.0],\n",
    "        [1.0, 2.0],\n",
    "        [1.0, 3.0],\n",
    "    ]\n",
    ")\n",
    "X\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model(X)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 Multiple Linear Regression Training\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "l\\left(\\mathbb{w}, b\\right)=\n",
    "\\frac{1}{N}\n",
    "\\sum_{i=1}^N{\n",
    "  \\left(\n",
    "    y_i - (x \\cdot \\mathbb{w}+b)\n",
    "  \\right)^2\n",
    "}\n",
    "$$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gradient vector\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\nabla l(\\mathbb{w}, b) = \\begin{bmatrix}\n",
    "  \\frac{\\partial}{\\partial w_1}l(\\mathbb{w}, b) \\\\\n",
    "  \\frac{\\partial}{\\partial w_2}l(\\mathbb{w}, b) \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial}{\\partial w_d}l(\\mathbb{w}, b) \\\\\n",
    "  \\frac{\\partial}{\\partial b}l(\\mathbb{w}, b) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Data2D(torch.utils.data.Dataset):\n",
    "  def __init__(self, n_samples=21, ndim=2, xlim=1, noise=0.1):\n",
    "\n",
    "    self.x = torch.zeros(n_samples, ndim)\n",
    "    for idim in range(ndim):\n",
    "      self.x[:, idim] = torch.linspace(-xlim, xlim, n_samples)\n",
    "\n",
    "    self.w = torch.tensor([[1.0]]*ndim)\n",
    "    self.b = 1.0\n",
    "\n",
    "    self.f = self.forward(self.w, self.x, self.b)\n",
    "    self.y = self.x + noise * torch.randn((n_samples, 1))\n",
    "\n",
    "    self.len = n_samples\n",
    "\n",
    "  @staticmethod\n",
    "  def forward(w, x, b):\n",
    "    return torch.mm(x, w) + b\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.x[index], self.y[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.len\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data_set = Data2D()\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    dataset=data_set,\n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "model = LR(input_size=2, output_size=1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "LOSS, epoch_list = [], []\n",
    "\n",
    "for epoch in range(100):\n",
    "  for k, (x, y) in enumerate(trainloader):\n",
    "    yhat = model(x)\n",
    "    loss = criterion(yhat, y)\n",
    "\n",
    "    epoch_list.append(epoch + k/len(trainloader))\n",
    "    LOSS.append(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.loglog(epoch_list, LOSS, '.-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Multiple dimension example :\n",
    "* input 1 dimension\n",
    "* output 10 dimension\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "model = LR(1, 10)\n",
    "model(torch.tensor([1.0]))\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "list(\n",
    "    model.parameters()\n",
    ")\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[1.0]])\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predict one sample\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "yhat = model(x)\n",
    "yhat\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predict three samples\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "X = torch.tensor([\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [3.0],\n",
    "])\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Yhat = model(X)\n",
    "Yhat\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\mathbb{y}_{n \\times m} = \\mathbb{X}_{n \\times d}\\mathbb{W}_{d \\times m} + b\n",
    "$$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training with multiple output\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\mathbb{y}_{n \\times m} = \\mathbb{X}_{n \\times d}\\mathbb{W}_{d \\times m} + \\mathbb{b}_{-1\\times m} \\\\\n",
    "l({\\mathbb{W}}, b) = \\frac{1}{N}\\sum_{i=1}^{N}{\n",
    "  \\left\\Vert\n",
    "    y_i-(\\mathbb{x}_i\\mathbb{W}_{d \\times m}+\\mathbb{b})\n",
    "  \\right\\Vert^2\n",
    "}\n",
    "$$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Updating the Weight $\\mathbb{W}$ and bias $\\mathbb{b}$ from k'th step to (k+1)st\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\mathbb{W}^{k+1} &= \\mathbb{W}^k-\\eta \\nabla l(\\mathbb{W}^k, b^k) \\\\\n",
    "  \\mathbb{b}^{k+1} &= \\mathbb{b}^k-\\eta \\frac{\\partial}{\\partial b} l(\\mathbb{W}^k, b^k)\n",
    "\\end{align}\n",
    "$$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`Data2D` class for multiple input dimensions and multiple output dimensions\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class Data2D(torch.utils.data.Dataset):\n",
    "  def __init__(self, n_samples=21, in_dim=2, out_dim=2, xlim=1, noise=0.1):\n",
    "    self.x = torch.zeros(n_samples, in_dim)\n",
    "    for idim in range(in_dim):\n",
    "      self.x[:, idim] = torch.linspace(-xlim, xlim, n_samples)\n",
    "\n",
    "    self.w = torch.tensor(\n",
    "      [\n",
    "        [float((-1)**odim) for odim in range(out_dim)]\n",
    "          for idim in range(in_dim)\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    self.b = torch.tensor(\n",
    "      [\n",
    "        [float((-1)**odim) for odim in range(out_dim)]\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    self.f = self.forward(self.w, self.x, self.b)\n",
    "    self.y = self.x + noise * torch.randn((n_samples, 1))\n",
    "\n",
    "    self.len = n_samples\n",
    "\n",
    "  @staticmethod\n",
    "  def forward(w, x, b):\n",
    "    return torch.mm(x, w) + b\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.x[index], self.y[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.len\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training steps for MIMO\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "n_input_dimension = 2\n",
    "n_output_dimensin = 2\n",
    "n_epoch = 100\n",
    "n_batch = 5\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Traning using Mini-Batch Gradient Descent\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "data_set = Data2D()\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    dataset=data_set,\n",
    "    batch_size=n_batch\n",
    ")\n",
    "\n",
    "model = LR(input_size=n_input_dimension, output_size=n_output_dimensin)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "LOSS, epoch_list = [], []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "  for k, (x, y) in enumerate(trainloader):\n",
    "    yhat = model(x)\n",
    "    loss = criterion(yhat, y)\n",
    "\n",
    "    epoch_list.append(epoch + k/len(trainloader))\n",
    "    LOSS.append(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.loglog(epoch_list, LOSS, '.-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.0 Linear Classifiers\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Linear Classifiers\n",
    "* Will always give continuous numbers : 3, -2, 0, ...\n",
    "* To match with class that we want to fit, would need **Threshold Functions**\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic Regression for classification\n",
    "* Would use sigmoid function\n",
    "$$\n",
    "\\delta(z)=\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "* Would smoothly connect 0 and 1\n",
    "* Pass linear function result through $\\delta(z)$ to get classification\n",
    "* Possible to consider as a probability\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1 Logistic Regression : Prediction\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic Function\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* scalar $x$\n",
    "$$\n",
    "\\begin{align}\n",
    "z &= wx + b \\\\\n",
    "\\hat y &= \\sigma(z)\n",
    "\\end{align}\n",
    "$$\n",
    "* vector $\\mathbb x$\n",
    "$$\n",
    "\\begin{align}\n",
    "z &= \\mathbb x\\mathbb w + b \\\\\n",
    "\\hat y &= \\sigma(z)\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get `sig()` function from `nn.Sigmoid()` and plot $z$ vs $\\sigma(z)$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z = torch.linspace(-100, 100, 2000+1).view(-1, 1)\n",
    "\n",
    "sig = torch.nn.Sigmoid()\n",
    "\n",
    "yhat = sig(z)\n",
    "\n",
    "plt.plot(z.numpy(), yhat.numpy())\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('$\\sigma(z)$')\n",
    "plt.grid(True)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot $z$ vs $\\sigma(z)$ using `torch.sigmoid()`\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z = torch.linspace(-100, 100, 2000+1).view(-1, 1)\n",
    "yhat = torch.sigmoid(z)\n",
    "\n",
    "plt.plot(z.numpy(), yhat.numpy())\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('$\\sigma(z)$')\n",
    "plt.grid(True)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### `nn.Sequential()`\n",
    "* To simplify implementing logistic regression model\n",
    "* Pass both return values from `nn.Linear()` and `nn.Sigmoid()` to `nn.Sequential()`\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "sequential_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "yhat = sequential_model(z)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Build a custom model using `nn.Module`\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn\n",
    "\n",
    "\n",
    "class logistic_regression(torch.nn.Module):\n",
    "  def __init__(self, in_size, out_size=1):\n",
    "    super(logistic_regression, self).__init__()\n",
    "    self.linear = torch.nn.Linear(in_size, out_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return torch.sigmoid(self.linear(x))\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "custom_model = logistic_regression(1)\n",
    "yhat = custom_model(z)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Making a Prediction\n",
    "parameters : weigt and bias\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(list(sequential_model.parameters()))\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "single sample\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[1.0]])\n",
    "\n",
    "yhat = sequential_model(x)\n",
    "yhat\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "multiple samples\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.tensor([\n",
    "    [1.0],\n",
    "    [100.0],\n",
    "])\n",
    "\n",
    "yhat = sequential_model(x)\n",
    "yhat\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Multidimensional Logistic Regression\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Input dimension = 2\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "custom_2d_model = logistic_regression(2)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "or\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "sequential_2d_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 1),\n",
    "    torch.nn.Sigmoid(),\n",
    ")\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(list(sequential_model.parameters()))\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "single sample\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[1.0, 1.0]])\n",
    "\n",
    "yhat = sequential_2d_model(x)\n",
    "yhat\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "multiple samples\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.tensor([\n",
    "    [1.0, 1.0],\n",
    "    [1.0, 2.0],\n",
    "    [1.0, 3.0],\n",
    "])\n",
    "\n",
    "yhat = sequential_2d_model(x)\n",
    "yhat\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2 Bernoulli Distribution Maximum Likelhood Estimation (MLE)\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Biased coin flip\n",
    "$\\theta = 0.2$\n",
    "\n",
    "face | outcome | probability\n",
    ":-----:|:-----:|:-----:\n",
    "heads | `y = 1` | $$P(1)=0.2$$\n",
    "tails | `y = 0` | $$P(0)=0.8$$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "likelihood\n",
    "\n",
    "head | head | tail | likelihood\n",
    ":----:|:----:|:----:|:----:\n",
    "$$\\theta$$ | $$\\theta$$ | $$1-\\theta$$ | $$\\theta^2 (1-\\theta) $$\n",
    "0.2 | 0.2 | 0.8 | 0.032\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bernoulli distribution\n",
    "$$\n",
    "p(y|\\theta) = \\theta ^ y (1 - \\theta) ^ {1-y}\n",
    "$$\n",
    "\n",
    "face | outcome | probability\n",
    ":-----:|:-----:|:-----:\n",
    "heads | `y = 1` | $$p(y=1|\\theta)=\\theta ^ 1(1-\\theta)^{1-1} = \\theta$$\n",
    "tails | `y = 0` | $$p(y=0|\\theta)=\\theta ^ 0(1-\\theta)^{1-0} = (1-\\theta)$$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Likelihood function\n",
    "$$\n",
    "p(Y|\\theta)=\\prod_{n=1}^{N}{p\\left(y_n|\\theta\\right)}\n",
    "=\\prod_{n=1}^{N}{\\theta ^ {y_n} (1 - \\theta) ^ {1-y_n}} \\\\\n",
    "$$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Maxizing likelihood\n",
    "$$\n",
    "\\hat \\theta = \\underset{\\theta}{argmax}(P(Y|\\theta))\n",
    "$$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Maximizing the Log likelihood function\n",
    "$$\n",
    "\\hat \\theta = \\underset{\\theta}{argmax}(ln(P(Y|\\theta)))\n",
    "$$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Log likelihood function\n",
    "$$\n",
    "l(\\theta)\n",
    "=ln(p(Y|\\theta))\n",
    "=\\sum_{n=1}^{N}{y_n ln\\theta +(1-y_n) ln(1 - \\theta)}\n",
    "$$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.3 Logistic Regression Cross Entropy Loss\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Problems with Mean Square Error (MSE)\n",
    "* Can be flat in some regions\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cost function\n",
    "$$\n",
    "l(w, b) = \\frac{1}{N}\\sum_{i=1}^{N}{\n",
    "  \\left(\n",
    "    y_i-\\sigma(w x_i+b)\n",
    "  \\right)^2\n",
    "}\n",
    "$$\n",
    "Assume $w = 1$ to simplify\n",
    "$$\n",
    "l(b) = \\frac{1}{N}\\sum_{i=1}^{N}{\n",
    "  \\left(\n",
    "    y_i-\\sigma(x_i+b)\n",
    "  \\right)^2\n",
    "}\n",
    "$$\n",
    "Also consider the threshold function\n",
    "$$\n",
    "l(b) = \\frac{1}{N}\\sum_{i=1}^{N}{\n",
    "  \\left(\n",
    "    y_i-THR(x_i+b)\n",
    "  \\right)^2\n",
    "}\n",
    "$$\n",
    "$b$ of the next training step\n",
    "$$\n",
    "b^2=b^1 - \\eta \\frac{d}{db}l(b^1)\n",
    "$$\n",
    "* With the threshold function, the cost function slope can be flat\n",
    "* With the sigmoid $\\sigma()$ function, the cost function would have slope\n",
    "* Thus it will be easier to train\n",
    "* Even so, in a higher dimensional space, some regions of the MSE cost functions may have flat slope\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Maximum Likelihood\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic Regression Cross Entropy\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PyTorch\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}